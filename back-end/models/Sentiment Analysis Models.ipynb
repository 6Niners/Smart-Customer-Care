{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# from Text_Normalization import Text_Normalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU , Conv1D, MaxPool1D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all data and export to csv\n",
    "\n",
    "# df = pd.read_csv('Data - Dirty.csv')\n",
    "# df = Text_Normalization(df,\"tweets\")\n",
    "# df.to_csv(\"Data - Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Test if GPU is Detected\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>حقوق المرأة</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حقوق المرأة الإسلام</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لجنة التنمية بشبرا ما زال التسجيل مستمر دورة ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حقوق المرأة التي تضمنها وزارة العدل</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ولي امر الزوجة ولي الزوجة ولي المراة الاخطاء ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label\n",
       "0                                       حقوق المرأة       1\n",
       "1                               حقوق المرأة الإسلام       1\n",
       "2   لجنة التنمية بشبرا ما زال التسجيل مستمر دورة ...      1\n",
       "3               حقوق المرأة التي تضمنها وزارة العدل       1\n",
       "4   ولي امر الزوجة ولي الزوجة ولي المراة الاخطاء ...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data - Cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[' نبي حل عطالة الصيادله متخرجين سنوات وبالالاف عاطلين ان احتياج ملحوظ ونشوف اغلب مراكزنا الصحيه معطين الصيدليه للتمريض خسارة التعب ٦ سنوات وزير الصحه الصوره ', 0], [' سعوديات نطلب اسقاط الولايه يا جماعة اخاطب عنده منطق ليه الولاية ما تسقط المرأة العاقلة ليه يكون أمرها تحت تصرف شخص آخر يعني صار هوس السعوديات فترة ما انها تبغى تتزوج عشان تفتك سجن ابوها وبعدين تكتشف انها بمعتقل زوجها ليه تخلون البنت تتعلق بأشخاص وتضيع طموحاتها ', 1], [' سعوديات نطلب اسقاط الولايه وحنا نطلب تحديد سن للرشد وإلغاء الولاية وش الصعب والمستحيل هذه البديهيات الحقوق تسسقط ', 1]]\n"
     ]
    }
   ],
   "source": [
    "# Reformat to a list\n",
    "\n",
    "fullDataList = df.values.tolist()\n",
    "random.seed(5)\n",
    "random.shuffle(fullDataList)\n",
    "\n",
    "print(\"\\n\" , fullDataList[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSize = int(len(fullDataList) * 0.8)  # 3357 from 4197\n",
    "maxSentenceLength = int(df[\"tweets\"].str.split().str.len().mean()) # 20\n",
    "embeddingDim = 256\n",
    "truncType = \"post\"\n",
    "paddingType = \"post\"\n",
    "\n",
    "learningRate = 0.0001\n",
    "optimizer = Adam(learningRate)\n",
    "lossFunc = \"binary_crossentropy\"\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into x and y - (data, result) - (sentence, sentiment)\n",
    "sentences = []\n",
    "sentimentScores = []\n",
    "\n",
    "for xyPair in fullDataList:\n",
    "    sentences.append(xyPair[0])\n",
    "    sentimentScores.append(xyPair[1])\n",
    "\n",
    "\n",
    "#Tokenized the sentences into numbers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "\n",
    "# Compiled the total number of words after tokenization\n",
    "wordIndex = tokenizer.word_index \n",
    "vocabSize = len(wordIndex) + 1\n",
    "\n",
    "\n",
    "# Same as tokenization step but with sequences of words + added padding and truncating to data with inconsistent lengths\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "paddedSequences = pad_sequences(sequences, maxlen=maxSentenceLength, truncating=truncType, padding=paddingType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test sets\n",
    "\n",
    "trainSequences = paddedSequences[0:trainingSize]\n",
    "trainSentiments = sentimentScores[0:trainingSize]\n",
    "\n",
    "testSequences = paddedSequences[trainingSize:]\n",
    "testSentiments = sentimentScores[trainingSize:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training Sequences:  3357\n",
      "Number of training Sentiments:  3357\n",
      "Number of Testing Sequences:  840\n",
      "Number of Testing Sentiments:  840\n",
      "\n",
      " Random padded sequence:\n",
      " [  18   19  688 2019 2020 8688    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training Sequences: \", len(trainSequences))\n",
    "print(\"Number of training Sentiments: \", len(trainSentiments))\n",
    "print(\"Number of Testing Sequences: \", len(testSequences))\n",
    "print(\"Number of Testing Sentiments: \", len(testSentiments))\n",
    "\n",
    "print(\"\\n Random padded sequence:\\n\", paddedSequences[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from glove file data {word : Vector values}\n",
    "\n",
    "gloveFileName = \"./Arabic_GloVe.txt\"\n",
    "embeddingIndex = {}\n",
    "\n",
    "with open(gloveFileName, encoding=\"utf-8\") as glove:\n",
    "    \n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingIndex[word] = coefs\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24041, 256)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter words in glove file to existing words in our data\n",
    "\n",
    "embeddingMatrix = np.zeros((vocabSize, embeddingDim))\n",
    "\n",
    "for word,i in wordIndex.items():\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector \n",
    "\n",
    "\n",
    "\n",
    "embeddingMatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 256)           6154496   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 20, 256)           0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 15)                12285     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 15)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,166,797\n",
      "Trainable params: 12,301\n",
      "Non-trainable params: 6,154,496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocabSize, embeddingDim, input_length=maxSentenceLength, weights=[embeddingMatrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# model.add(Conv1D(16, 3, activation=\"relu\"))\n",
    "# model.add(MaxPool1D(pool_size=4))\n",
    "# model.add(LSTM(15))\n",
    "model.add(GRU(15))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=lossFunc, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpPath=\"./lstm_weights.h5\"\n",
    "cpPath=\"./gru_weights.h5\"\n",
    "# cpPath=\"./conv_weights.h5\"\n",
    "# cpPath=\"./hybrid_lstm_weights.h5\"\n",
    "# cpPath=\"./hybrid_gru_weights.h5\"\n",
    "\n",
    "cpCallback = ModelCheckpoint(cpPath, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=np.array(trainSequences),\n",
    "                    y=np.array(trainSentiments),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(np.array(testSequences), np.array(testSentiments)), \n",
    "                    callbacks=[cpCallback],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - 5s - loss: 0.3349 - accuracy: 0.8619 - 5s/epoch - 198ms/step\n",
      "Restored model, accuracy: 86.19%\n"
     ]
    }
   ],
   "source": [
    "testSequenceArray = np.array(testSequences)\n",
    "testSentimentArray = np.array(testSentiments)\n",
    "\n",
    "model.built = True\n",
    "model.load_weights(cpPath)\n",
    "\n",
    "loss, acc = model.evaluate(testSequenceArray, testSentimentArray, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAndPrintSentiment(sample):\n",
    "    sentimentsList = [\"Negative\", \"Positive\"]\n",
    "\n",
    "    predictSequence = tokenizer.texts_to_sequences(sample)\n",
    "    paddedPredictSequences = pad_sequences(predictSequence, maxlen=maxSentenceLength, truncating=truncType, padding=paddingType)\n",
    "\n",
    "    predictionValue = int(model.predict(paddedPredictSequences)[0][0].round())\n",
    "\n",
    "    prediction = sentimentsList[predictionValue]\n",
    "\n",
    "    print(\"Score:\", model.predict(paddedPredictSequences)[0][0])\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "valAcc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.plot(epochs, valAcc, 'b')\n",
    "plt.title('Training and Test accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "prediction = model.predict(testSequenceArray)\n",
    "y_pred = [1 if x>0.5 else 0 for x in prediction]\n",
    "cm = confusion_matrix(testSentimentArray, y_pred)\n",
    "\n",
    "\n",
    "labels = [\"0\", \"1\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1_score, _ =  precision_recall_fscore_support(testSentimentArray, y_pred, average='macro')\n",
    "print('Recall = ', recall)\n",
    "print('Precision = ', precision)\n",
    "print('F1 Score = ',f1_score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f984edf3630f16ee1008b26898737844d661a2f1dde9207003bbd3c089120118"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
