{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bda069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in d:\\anaconda\\lib\\site-packages (4.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in d:\\anaconda\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in d:\\anaconda\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f22780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: configparser in d:\\anaconda\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c00125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyenchant in d:\\anaconda\\lib\\site-packages (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8b6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f249ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read configs\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "api_key = config[\"twitter\"][\"api_key\"]\n",
    "api_key_secret = config[\"twitter\"][\"api_key_secret\"]\n",
    "\n",
    "access_token = config[\"twitter\"][\"access_token\"]\n",
    "access_token_secret = config[\"twitter\"][\"access_token_secret\"]\n",
    "\n",
    "# for testing \n",
    "#print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154c38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ed437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a782a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scedule\n",
      "ERROR: No matching distribution found for scedule\n"
     ]
    }
   ],
   "source": [
    "pip install scedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476468e",
   "metadata": {},
   "source": [
    "## From now that what we will work on and store in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc1a88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: place\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every 15 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n",
      "Unexpected parameter: place\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2 columns passed, passed data had 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;31m# caller's responsibility to check for this...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m             raise AssertionError(\n\u001b[0m\u001b[0;32m    693\u001b[0m                 \u001b[1;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 2 columns passed, passed data had 3 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bf2affbf1905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m#creat DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Hasht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;31m# testing the data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#df2.head(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    568\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# columns if columns is not None else []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         return _list_of_dict_to_arrays(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 2 columns passed, passed data had 3 columns"
     ]
    }
   ],
   "source": [
    "columns = [\"Tweet\",\"Time\"]\n",
    "data_Hasht = []\n",
    "data_tweet = []\n",
    "data_acc = []\n",
    "\n",
    "while '1' == '1':\n",
    "    '''\n",
    "    Your script that should loop here\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # search for an Hashtags\n",
    "    keywords_Hashtags = \"#فودافون\"\n",
    "\n",
    "    #search for an tweets\n",
    "    keywords_tweets = \"فودافون\"\n",
    "\n",
    "    #search for an account\n",
    "    keywords_account = \"@فودافون\"\n",
    "    \n",
    "\n",
    "    limit = 5000\n",
    "\n",
    "    tweets_Hashtags = tweepy.Cursor( api.search_tweets,q=keywords_Hashtags, count=1000, tweet_mode=\"extended\",lang=\"ar\",place= \"cairo,egypt\").items(limit)\n",
    "    tweets_tweets = tweepy.Cursor( api.search_tweets,q=keywords_tweets, count=1000, tweet_mode=\"extended\",lang=\"ar\",place= \"cairo,egypt\").items(limit)\n",
    "    tweets_account = tweepy.Cursor( api.search_tweets,q=keywords_account, count=1000, tweet_mode=\"extended\",lang=\"ar\",place= \"cairo,egypt\").items(limit)\n",
    "    #twe = tweepy.Cursor(api.search_tweets,q=keywords_Hashtags,lang=\"en\",since_id=\"2018-01-01\",until=\"2021-12-01\" ,result_type=\"recent\", tweet_mode=\"extended\").items(10)\n",
    "    print(\"every 15 min\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get all tweets text in the user home page timeline and append it in CSV file\n",
    "    \n",
    "    for tweet_Hasht in tweets_Hashtags:\n",
    "        data_Hasht.append([tweet_Hasht.full_text,tweet_Hasht.created_at,tweet_Hasht.user.location])\n",
    "        # testing for appending in the right way in the list\n",
    "        #print(tweet_Hasht.full_text)\n",
    "    \n",
    "    \n",
    "    for tweet_tweet in tweets_tweets:\n",
    "        data_tweet.append([tweet_tweet.full_text,tweet_tweet.created_at, tweet_tweet.user.location])\n",
    "        # testing for appending in the right way in the list\n",
    "        #print(tweet_tweet.full_text)\n",
    "\n",
    "    for tweet_acc in tweets_account:\n",
    "        data_acc.append([tweet_acc.full_text,tweet_acc.created_at, tweet_acc.user.location])\n",
    "        # testing for appending in the right way in the list\n",
    "        #print(tweet_acc.full_text)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #creat DataFrame\n",
    "    df2 = pd.DataFrame(data_Hasht, columns=columns)\n",
    "    # testing the data frame\n",
    "    #df2.head(3)\n",
    "\n",
    "\n",
    "    #creat DataFrame\n",
    "    df3 = pd.DataFrame(data_tweet, columns=columns)\n",
    "    # testing the data frame\n",
    "    #df3.head(3)\n",
    "\n",
    "\n",
    "    #creat DataFrame\n",
    "    df4 = pd.DataFrame(data_acc, columns=columns)\n",
    "    # testing the data frame\n",
    "    #df4.head(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    frames = [df2, df3,df4]\n",
    "  \n",
    "    df = pd.concat(frames)\n",
    "    \n",
    "    df.to_csv(\"Scrapping results before cleanning.csv\")\n",
    "    \n",
    "    print(df)\n",
    "    time.sleep(1800)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e460923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tweets text in the user home page timeline and append it in CSV file\n",
    "columns = [\"Tweet\",\"Time\"]\n",
    "data_Hasht = []\n",
    "data_tweet = []\n",
    "data_acc = []\n",
    "\n",
    "\n",
    "for tweet_Hasht in tweets_Hashtags:\n",
    "    data_Hasht.append([tweet_Hasht.full_text,tweet_Hasht.created_at])\n",
    "    # testing for appending in the right way in the list\n",
    "    print(tweet_Hasht.full_text)\n",
    "    \n",
    "    \n",
    "for tweet_tweet in tweets_tweets:\n",
    "    data_tweet.append([tweet_tweet.full_text,tweet_tweet.created_at])\n",
    "    # testing for appending in the right way in the list\n",
    "    print(tweet_tweet.full_text)\n",
    "\n",
    "for tweet_acc in tweets_account:\n",
    "    data_acc.append([tweet_acc.full_text,tweet_acc.created_at])\n",
    "    # testing for appending in the right way in the list\n",
    "    print(tweet_acc.full_text)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat DataFrame\n",
    "df2 = pd.DataFrame(data_Hasht, columns=columns)\n",
    "# testing the data frame\n",
    "#df2.head(3)\n",
    "\n",
    "\n",
    "#creat DataFrame\n",
    "df3 = pd.DataFrame(data_tweet, columns=columns)\n",
    "# testing the data frame\n",
    "#df3.head(3)\n",
    "\n",
    "\n",
    "#creat DataFrame\n",
    "df4 = pd.DataFrame(data_acc, columns=columns)\n",
    "# testing the data frame\n",
    "#df4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df2, df3,df4]\n",
    "  \n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Scrapping results before cleanning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceccc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd3ee4",
   "metadata": {},
   "source": [
    "# Start of data cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations like . , ! $( ) * % @ from the text\n",
    "df['Tweet after cleanning'] = df['Tweet'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet after cleanning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4304680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the zeros after the time and split the time column to time and date columns\n",
    "dates = []\n",
    "times = []\n",
    "\n",
    "years =[]\n",
    "months = []\n",
    "days =[]\n",
    "hours = []\n",
    "minuites = []\n",
    "for i in df['Time']:\n",
    "    hours.append(i.time().hour)\n",
    "    minuites.append(i.time().min)\n",
    "    \n",
    "    years.append(i.date().year)\n",
    "    months.append(i.date().month)\n",
    "    days.append(i.date().day)\n",
    "    \n",
    "    \n",
    "    #time.append(res[1])\n",
    "df['year']=years\n",
    "df['month']=months\n",
    "df['day']=days\n",
    "\n",
    "df['hour']=hours\n",
    "df['min']=minuites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c156825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Scrapping results after cleanning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd797b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd7502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "\"\"\"consumerKey = \"CONSUMER_KEY\"\n",
    "consumerSecret = \"CONSUMER_SECRET\"\n",
    "accessToken = \"ACCESS_TOKEN\"\n",
    "accessTokenSecret = \"ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\"\"\"\n",
    "# search for an Hashtags\n",
    "keywords_Hashtags = \"#فودافون\"\n",
    "limit = 5000\n",
    "tweets_Hashtags = tweepy.Cursor( api.search_tweets,q=keywords_Hashtags, count=100, tweet_mode=\"extended\").items(limit)\n",
    "\n",
    "\n",
    "username = sys.argv[1]\n",
    "startDate = datetime.datetime(2011, 6, 1, 0, 0, 0)\n",
    "endDate =   datetime.datetime(2012, 1, 1, 0, 0, 0)\n",
    "\n",
    "tweets = []\n",
    "tmpTweets = api.user_timeline(username)\n",
    "for tweet in tmpTweets:\n",
    "    if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "        tweets.append(tweet)\n",
    "\n",
    "while (tmpTweets[-1].created_at > startDate):\n",
    "    tmpTweets = api.user_timeline(username, max_id = tmpTweets[-1].id)\n",
    "    for tweet in tmpTweets:\n",
    "        if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "            tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c14a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d57a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700b28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f041f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "word = \"Bonjour\"\n",
    "d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hallo احمد\"\n",
    "english_words = []\n",
    "for word in text:\n",
    "    if d.check(word) == False:\n",
    "        english_words.append(word)[10]\n",
    "        \n",
    "    print(english_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
